Would you, please, show the possible pitfalls of splitting the ML dataset into detailed samples? Details. Task - Machine learning binnary classification (probability of buying the product by the client). Tool - Gradient Boosting (XGBoost) Number of features - 20 Current Dataset (Number of samples - 100 000) Unique_client_id feature1 target 1 20 1 2 23 0 Desirable Dataset (Number of samples - 1 000 000) Unique_client_id client_phone feature1 target 1 1 32 1 1 2 22 0 2 1 23 0 Thus, I am going to predict the target value not for client, but for phone number.
One client may have 1 phone number and another - 100 phone numbers.
The client with 100 phone numbers will have 100 rows in the dataset. Some features, which belong to the client (not to the phone, e.g. age) are repeated in 100 rows.
The client with one phone number will have one row in the dataset. For now, I see the only pitfall: in the case of lack information in the phone number features, the client features will force them out.