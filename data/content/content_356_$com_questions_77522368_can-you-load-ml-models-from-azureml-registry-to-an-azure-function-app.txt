Is it possible to load  registered ml models from azureml model registry to an azure function app and carry out inference or do these models need to be downloaded inside the function app first? any examples or guides? model.predict > predict is not directly available when you load the model from Azure ML workspace, As you might face the error below:- Thus its necessary to download the model via joblib or other library in the memory and then perform the inference. For authentication you can directly use Service Principal authentication in your function app code:- My Python Function v1, init.py :- Request Body:- Output:- Azure Function Output:- But if you do not want to download the model, You can create one scoring script and dependencies then run your inference. scoring.py :- And then use this scoring script for your inference, Refer below:-