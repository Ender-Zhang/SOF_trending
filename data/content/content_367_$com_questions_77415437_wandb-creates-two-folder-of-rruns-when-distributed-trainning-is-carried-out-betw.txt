I am new to many things in deep learning and distributed training. I have defined a function for the distributed training setting: For the above function, I followed the example in wandb and also for the training and validation datasets, I followed the example : and the model initialization is as follows: and the wandb initialization is: My question is why when training the model, wandb creates two different run folders with two models, where I have two gpus? This is not correct setting for my experiments. should not be only one trained model when distributed into two gpus? Thanks