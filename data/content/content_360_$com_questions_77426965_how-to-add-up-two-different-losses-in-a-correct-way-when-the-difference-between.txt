I have two different losses for training the model. L1 loss is computed as: and another customized loss function for my model, that computes the loss for two 3D data cubes, new_criterion = MyLoss(...) . These two losses are added with weight: The output of two-loss functions is as follows: It seems the difference between two loss values is quite big and giving a lower weight to F.L1 (recon_loss), it becomes smaller. Is this value difference (i.e. smaller value (2.4668e-05) vs. larger value (0.8987), and then adding them together to obtain loss_total will affect the training? If yes, how do we resolve this and make sure the model trains well, benefiting from both losses? How normally two different losses are added up together to train a model? I work with some architecture which use several losses simultaneously and they adjust the weights for each loss function based on their importance and scale. you can see example fastpitch model