Currently, I'm working on a project that listens to every API request, perform checks on it and sends it back. Sadly, API managed to overload my single threaded AsyncIO and multi-threading attempts, but CPU never reached even 20% of load. So my idea is to use multiprocessing and fully load (70-90%) the CPU. Best way (I thought) was to use concurrent.futures.ProcessPoolExecutor or multiprocessing.Pool as I could simply set max_workers and max_tasks_per_child and with power of asyncio and handle as many requests as CPU could possibly handle. But I failed. My current problem is with process creation inside my custom Queue system. To be exact, I don't exactly know how to create an isolated process that would: spawn an asyncio loop run async function (with 3s timeout) that has asyncio.TaskGroup() run all tasks return data basically, I want to create a copy of required check functions (Queue system provides them), run them in the isolated process and return data. Here is the add_queue method: and the method that gets called: Both of these methods provided, are inside a class named Queue . After running this, nothing happened. Figured out that .submit() did not execute provided function, but executed main.py which only has the whole system start-up. Queue system is in src.core.transmitters.queue . How can I create an isolated processes, that creates a copy of the runner function ( process_event_runner ) and all the other functions that are defined inside a tuple in functions variable and runs them? And still have a way to limit workers and a way to use asyncio ? Is it even possible? For any questions on how the whole system works, feel free to ask in the comments and I will try my best to answer them. As asked by the Michael in the comments, here is the full traceback of the circular import error that I get when trying to import the specified function (process_event_runner) in main.py before if __name__ == "__main__" : The loader is after the if __main__... rule and start method is set to spawn . Like this: