I want to calculate the term A * B * A' for matrices A and B . The A' is the transpose of A . Is there an efficient way to calculate this on Python? I can do A @ B @ A.T yet I want  something: Which takes advantage of the symmetric for calculation. Guarantees a symmetric result. I have the most straight forward numba based code: The output is symmetric.
Could we do better performance wise? I also thought about fixing the symmetry of the numpy result by: I am looking for the fastest solution for small to medium sized matrices. Each dimension in the range 2-100. The provided code run in O(n**4) while two matrix multiplication runs in O(n**3) . Thus, 2 matrix multiplications are certainly significantly faster. One can try to change the order of the loop and then factorize some computation but the result will probably be similar to 2 multiplication matrix. A simpler approach consists in writing the code performing 2 multiplication matrix, then optimizing by swapping loops to make it more SIMD-friendly, then adapt it to compute only the upper triangular part for the last multiplication matrix. The intermediate matrix can actually be computed line by line (which is a bit more efficient for large matrices). The lower triangular part can be computed like you did in the provided implementation. Here is the resulting code: This solution is significantly faster than the provided one. However, it is a bit slower than A @ B @ A.T . This is because Numpy uses BLAS libraries to compute matrix multiplications and the BLAS used on my machine is OpenBLAS : a highly-optimized implementation. OpenBLAS performs the matrix multiplication in parallel while the above Numba code is sequential. If you plan to run the Numba function from a multithreaded code, then the Numba code will be faster than the one of Numpy. Otherwise, you can parallelize the loop i that way: This solution is faster than the sequential code on most machine, except for very small matrices (since spawning threads, distributing the work and waiting for them is not free). However, it is still slower than the basic Numpy code A @ B @ A.T on my machine. This is because the allocation of line does not scale at all. Indeed, the parallel code is only 3.2 times faster than the sequential one on my 6-core CPU. AFAIK, there is no simple solution in Numba to fix this (known) issue yet. In Cython, the solution would be to use a stack allocation or a thread-local one to fix this issue. Still, even with a perfect scaling, the above code would be just 30% faster on 60x60 matrices. This shows how hard it is to outperform highly-optimized BLAS implementations and also that the symmetry only give a small performance gain for medium sized-matrices. Actually, only half of the second matrix multiplication needs to be computed (optimal), but the first certainly needs to be fully computed. Thus, the theoretical optimal gain is certainly 25% which is rather small. That being said, in practice, for very small matrices, the sequential Numba implementation should be significantly faster than the Numpy code (due to the threading overhead).