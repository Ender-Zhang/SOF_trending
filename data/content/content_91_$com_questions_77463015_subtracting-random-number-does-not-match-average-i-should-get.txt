Suppose you organize a competition with 25 million participants. At each round, a random number from the people remaining is eliminated. How many rounds should we expect to get 5 participants or less ? Now 25M/2^22 is the closest number higher than 5 so 22 rounds on average. However, when verifying this in Python, I get that the distribution is more accumulated around 15-16 rather than 22-23. (I am not very familiar with Python so I know I should use dictionaries to count stuff etc.) Is there a mistake in my code ? Because theory cannot lie. Expect about 16.33 rounds. Let e[X] be the expected number of rounds for starting with X participants. In Python: Output: Running your experiment three times, printing the average sum(lst) / len(lst) : That is the expected result. It happens because of the way the probabilities combine. You're expecting to reduce the value by half on average for each iteration, but that isn't actually what happens. Reducing the value by 10% in a single iteration has the same probability as reducing it by 90% in a single iteration. If the value is reduced by 10% seven times in a row, it will be reduced to about 48% of the original value. If it is reduced by 90% seven times in a row, the result is reduced to 0.00001% of the original value. In other words, the linear distributions of probabilities for what to reduce X by means that the times the reduction is high will have a greater impact than the times the reduction is low. In order to get an average number of iterations roughly equal to log2(25000000) , you need a logarithmic random distribution. Edit: I should clarify that this will happen with any decent PRNG. Just for sanity, I converted it to rust (code below) and it behaves the same way.