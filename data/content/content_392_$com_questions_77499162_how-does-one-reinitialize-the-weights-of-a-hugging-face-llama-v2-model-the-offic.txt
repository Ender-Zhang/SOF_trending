I want to reinitialize the weights of a LLaMA v2 model I'm using/downloading. I went through all the documentation and the source code from their Hugging  Face code: https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721 https://github.com/huggingface/transformers/blob/main/src/transformers/modeling_utils.py#L1154 https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L809 https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py#L721 docs https://huggingface.co/docs/transformers/main/model_doc/llama#transformers.LlamaModel and papers where neither of the two mentions how the initialized the models exactly or any of the layers llamav1 https://arxiv.org/pdf/2302.13971.pdf and llamav2 https://arxiv.org/pdf/2307.09288.pdf (maybe due to trade secrets?) I tried the very simple test of going through the modules/parameters and reinitializing according to how their code suggests and printing if the weights norm changed. It never changed, so I don't know if some mutation protection is going on in PyTorch Hugging  Face models. Is there something I might be doing wrong? And the output never showed the weight norms changed: What am I doing wrong? I need to know how to reinitialize the weights in the proper/correct way according to LLaMA. What exact init method and values should I use? Related related blog: https://blog.briankitano.com/llama-from-scratch/ Hugging Face question pre-train: https://discuss.huggingface.co/t/can-i-pretrain-llama-from-scratch/37821/7 Hugging Face Discord: https://discord.com/channels/879548962464493619/1174911090254172231/1174911090254172231 Hugging Face question reinit: https://discuss.huggingface.co/t/how-does-one-reinitialize-the-weights-of-a-hugging-face-llama-v2-model-the-official-way-as-the-original-model/62547 related: How to adapt LLaMA v2 model to less than 7B parameters? try with this example I see a possible problem in the main_reinit_model function. Specifically, you are trying to move the model to the GPU (cuda) before initializing the weights.