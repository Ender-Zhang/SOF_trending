As described in the title I am using SQLAlchemy and PYODBC in a python script to insert large csv files (up to 14GB) into a locally hosted SQL Server database. I know that I cannot host a 14gb dataframe in memory so I am using the chunk feature in pandas to run batch inserts and have experimented with batch sizes as small as 100 rows which easily fits into memory. I have a feeling the memory error is related to the SQL side of things. To minimize load processing I do not have any indexes on the tables I am inserting into (hash tables). No matter the batch size I am running out of memory at the same point in the loading process. What am I overlooking? Should I be flushing the memory somehow? Or is SQL Server waiting to commit the transaction until the connection closes? Here is my current code: Here is the stack trace: And the error message is simply "MemoryError". Further investigation revealed it is a specific file that is causing the error even if I don't load prior ones. I am starting to think it might be a bad error message. Thank you for all the suggestions. I don't see how there could be a memory error retrieving 100 rows with 6 columns from a csv. I will post if/when I find it. I don't know a lot of sqlserver but maybe try this to see if the entire import is trying to pile up in a single transaction, there might be a better way but this would be informative, (this would commit every batch):