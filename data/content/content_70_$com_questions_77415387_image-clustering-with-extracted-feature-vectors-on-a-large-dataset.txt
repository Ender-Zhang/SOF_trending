I've been trying to use a number of clustering techniques on an extremely large image dataset (around 1 million images). The process is quite standard - e.g., using a pretrained CNN (VGG19 in this case) to extract features and testing a number of clustering approaches (hierarchical and k-means at the moment) to group the images together. This works on a subset of my data. However, when I do this on my large dataset, I have two main problems: The majority of clustering examples in Python require an array as the input. My data is too large to fit into an array in memory (I have 64gb at the moment). I can process the features in VGG16 using the image generators/batch methods in keras, but then I cannot find a way to use these outputs in clustering algorithms (e.g., scipy's hierarchical clustering or sklearn clustering) without converting to an array. I essentially can't seem to figure out how to move those extracted features (which can easily be processed in batches) to a clustering method without using an array. The images actually come from around 4,000 'classes' but they are not entirely different across classes. The goal of this clustering method is to reduce those classes to much broader categories using unsupervised clustering - so I do need all the data, and I have no way to subset it (I have already used a test case from a random sample of images, now I'm trying to apply it to the whole dataset). Is there a way to get around using arrays for clustering packages like in scipy or sklearn, or should I be approaching this in a different way? I've tried looking at TensorFlow datasets, using dask and numpy mmemap, but I can't quite figure out how to take any of those objects/examples and process them using a clustering algorithm. In essence I've had some luck processing and storing the extracted feature as an output, but seem to be stuck at moving this into a clustering model/different model. I have also explored dimensionality reduction, but again I can't find anything to work conveniently with something like TF datasets where my processed features can be stored easily from the model. UPDATE: After some investigating and helpful comments, it seems the pairwise distance calculation is the problem which is required for some clustering algorithms. That still leaves me with a few I can use (Batch KMeans for example), although I'd be interested if there were other solutions to calculating pairwise distance over larger arrays/features.