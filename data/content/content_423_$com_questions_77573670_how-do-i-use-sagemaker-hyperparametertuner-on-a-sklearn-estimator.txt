I'm following an Amazon Sagemaker workshop to try and leverage several of Sagemaker's utilities instead of running everything off a Notebook as I'm currently doing. The thing is, in the workshop they teach you how to use HyperparameterTuner using the ready-made XGBoost image from AWS, while most of my pipelines are using Scikit-Learn models such as GradientBoostingClassifier or RandomForest, so I'm instantiating an estimator like this following this example file : After that, I am instantiating a HyperparameterTuner job by using the estimator I just created, with ranges for hyperparameters I want to test. My problem is that I haven't found ANY information on how to access the hyperparameters passed in the SKLearn estimator inside the "train.py" file. Nor have I found where do the optimal hyperparameters are stored so I can use them for the final model. Can someone tell if that's even possible, or offer alternatives if there's another easier way to do this?