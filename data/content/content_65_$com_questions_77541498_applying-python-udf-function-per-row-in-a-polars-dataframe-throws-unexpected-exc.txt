I have the following polars DF in Python I want to create a new column named "common_movie_ratings" that will take from each rating list only the index of the movie rated in the common movies. For example, for the first row, I should return only the ratings for movies [7064, 7153,], for the second row the ratings for the movie [7], and so on and so forth. For this reason, I created the following function: Finally, I apply the UDF function on the dataframe like Every time I apply the function, on the 3rd iteration/row I receive the following error expected tuple, got list I have also tried a different approach for the UDF function like But again on the 3rd iteration, I received the same error. Update - Data input and scenario scope ( here ) What went wrong with your approach Ignoring performance penalties for python UDFs, there are two things that went wrong in your approach. apply which is now map_rows in the context that you're trying to use it is expecting the output to be a tuple where each element of the tuple is an output column. Your function doesn't output a tuple. If you change the return line to return (ratings_for_common_movies,) then it outputs a tuple and will work. You can't add columns to polars dataframes with square bracket notation. The only thing that can be on the left side of the = is a df, never df['new_column']=<something> . If you're using an old version that does allow it then you shouldn't, in part, because new versions don't allow it. That means you have to do something like df.with_columns(new_column=<some_expression>) In the case adding a column to an existing df while using map_rows you can use hstack like: The above is really an anti-pattern as using any of the map_rows , map_elements , etc when a native approach could work will be slower and less efficient. Scroll to the bottom for a map_elements approach. Native solution preamble If we assume the lists are always 3 long then you could do this... Note that we're generating the indices list by looping through a range from 0 to the length of the list as n and when the item associated the n th position of user_movies is in common_movies then that n is put in the indices list. There is unfortunately not a .index like method in polars for list type columns so, without exploding the lists, this is the best way I can think of to create that indices list. Native solution answer Polars itself can't recursively set n_count so we need to do it manually. By using lazy evaluation this is faster than other approaches as it can compute each n_count case in parallel. By converting to lazy in the first part of the concat it allows for each frame to be calculated in parallel where each frame is a subset based on the length of the list. It also allows for the indices to become a CSER which means it only calculates it once even though there are 2 references to it. Incidentally, for less code but more processing/time, you could simply set n_counts in the preamble section to n_count=df.select(n_count=pl.col('user_movies').list.len().max()).item() and then just run the rest of what's in that section. That approach will be much slower than this one as, for every row it iterates through elements up to the max list length which adds unnecessary checks. It also doesn't get the same parallelism. In other words, it's doing more work with fewer CPU cores working on it. Benchmarks Fake data creation My method (16 threads): 1.92 s ± 195 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) My method (8 threads): 2.31 s ± 175 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) My method (4 threads): 3.14 s ± 221 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) @jqurious: 2.73 s ± 130 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) map_rows : 9.12 s ± 195 ms per loop (mean ± std. dev. of 7 runs, 1 loop each) Preamble with big n_count: 9.77 s ± 1.61 s per loop (mean ± std. dev. of 7 runs, 1 loop each) My method and the map_rows each use about 1 GB of RAM but the explody one is closer to 3 GB. struct and map_elements Instead of using map_rows which is, imo, really clunky, you can instead use map_elements . It has its own clunkiness as you often need to wrap your input in a struct but you can add columns more cleanly and you don't have to rely on column position. For instance you can define your function and use it as follows: What's happening here is that map_elements can only be invoked from a single column so if your custom function needs multiple inputs you can wrap them in a struct. The struct will get turned into a dict where the keys have the name of the columns. This approach doesn't have any inherent performance benefit relative to the map_rows , it's just, imo, better syntax. Lastly As @jqurious mentioned in comments of his answer, this could almost certainly be streamlined in terms of both syntax and performance by incorporating this logic with the formation of these lists. In other words, you have step 1: ______ step 2: this question. While I can only guess at what's happening in step 1 it is very likely that combining the two steps would be a worthwhile endeavor. [Update]: This approach uses ~30% less memory (but is slightly slower) As mentioned in the comments though, if you can share an example of the starting datasets, it's likely this can be done in a much simpler fashion. As for the performing the task you've described "natively": (i.e. without UDFs) You essentially need to reshape your data. You can then use Polars functionality e.g. .filter() + .is_in() to keep the matching rows.