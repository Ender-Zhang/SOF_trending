As part of my learning process I wanted to write a transformer model to copy input sequence to output sequence. I thought it would be relatively straight forward, however the results are less than ideal - loss is higher than I've expected resulting in inaccurate copies of numbers in the sequence. I would love to get some pointers on where I can improve (so far I tried a lot of hyper param tuning, nothing substantially changed the outcome). Is there something wrong with my architecture? Is there some algorithmic bugs I overlooked? My Implementation: Turns out encoder-decoder model is not suitable for this type of task. Using encoder-only model should work.