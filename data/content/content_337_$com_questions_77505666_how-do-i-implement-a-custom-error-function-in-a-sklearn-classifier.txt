I want to treat errors from overestimates and underestimates differently (like The Price is Right) during model training. I don't want to rewrite the entire MLP, regression, DecisionTree, etc. algorithms in sklearn just to implement my custom cost function and relevant derivative. Is there a way for me to define a function any classifier can use to override the default? This is an example of what I'm looking for: If I can't do this in sklearn but I have to use tensorflow or some other libraries, please let me know. In cases where you'd need to define a custom loss function, a neural-net framework would typically be used rather than sklearn . One usually can't supply a custom optimisation function to sklearn algorithms. If you wanted to stick with sklearn , some algorithms allow you to configure sample importance or class balancing, but from your question it doesn't seem like the desired solution. I don't want to rewrite the entire MLP, regression, DecisionTree, etc.
algorithms in sklearn just to implement my custom cost function and
relevant derivative. Not sure about decision trees, but MLPs and regression are straightforward to implement in PyTorch. Also, when you define a custom loss function, the derivative is taken care of for you. Here's a simple regression model using a custom loss function that penalises the overestimates more strongly than the underestimates: Some mock data for this example (2D feature space, and target is a scalar): Simple regression net with a custom loss function in PyTorch: For brevity this example leaves out details like scaling and batching the data (and keeping a validation set).