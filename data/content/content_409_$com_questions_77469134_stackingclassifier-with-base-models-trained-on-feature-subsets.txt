I can best describe my goal using a synthetic dataset. Suppose I have the following: After conducting a feature importance analysis, the discovered that each of the 3-classes in the dataset can best be predicted using feature subset, as oppose to the whole. For example: At this point, I would like to use 3 one-ve-rest classifiers to train sub-models, one for each class and using the class's best predictors (as the base models). And then a StackingClassifier for final prediction. I have high-level understanding of the StackingClassifier , where different base models can be trained (e.g. DT, SVC, KNN etc) and a meta classifier using another model e.g. Logistice Regression . In this case however, the base model is one DT classifier, only that each is to be trained using feature subset best for the class, as above. Then finally make predictions on the X_test . But I am not sure how this can be done. So I give the description of my work using pseudo data as above. How to design this to train the base models, and a final prediction? You can programmatically do what you describe but I am not sure what would be the gain over using a simple Random Forest that internally does all this (feature subselection and fitting etc). Here is an implementation of what you have described. I have used exactly the same base and stacking model as the ones you mentioned: You need to use make_pipeline and FunctionTransformer ( check this answer ) on top of custom functions to filter the data by the respective columns and to manipulate the target values. Here's a demo code