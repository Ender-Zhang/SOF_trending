When I was trying to add a perturbation to a model and optimize the perturbation itself but not the model parameters. I did like the following, which is very simple: However, the gradient of perturbation was still None after executing loss.backward(). I cannot figure out why. What caused this problem and what should I do to realize the result I needed? I think perturbation.grad shouldn't be None after executing loss.backward() but it was. Changing the .data attribute isn't tracked by autograd. You need something like return nn.functional.linear(x, perturbation + clean)