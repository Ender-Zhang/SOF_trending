I can get the perplexity of a whole sentence from here : But how can I get the perplexity value for each token, instead of of the average perplexity of the entire sequence of tokens? The input sentence in this example, 'Happy Birthday!' is composed of 3 tokens. Based on the formula for perplexity: This should result in 3 values: log probability of the first token, log probability of the second token given the first, and the log probability of the third token given the first 2. Each should be exponentiated to get the perplexity value of each token. I currently have the following: But I'm not sure what I'm doing wrong, as the last token seem to have the same perplexity as the entire sentence. this is happening because in the second code snippet, you loop over the input sequence by adding a new token at each iteration: Then, the computation of the perplexity in the last iteration of the loop is essentially identical to doing this: Here's how you can compute the perplexity and per-token perplexity (see https://github.com/huggingface/transformers/blob/v4.35.0/src/transformers/models/gpt2/modeling_gpt2.py#L1103 ): The output: