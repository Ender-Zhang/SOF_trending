Nvidia's GPU Performance Background User's Guide ( https://docs.nvidia.com/deeplearning/performance/dl-performance-gpu-background/index.html ) lists the following arithmetic intensities for fp16 data opreations. I'm trying to derive the AI math behind each of them. # Operation Arithmetic Intensity Usually limited by 1 Linear layer (4096 outputs, 1024 inputs, batch size 512) 315 FLOPS/B arithmetic 2 Linear layer (4096 outputs, 1024 inputs, batch size 1) 1 FLOPS/B memory 3 Max pooling with 3x3 window and unit stride 2.25 FLOPS/B memory 4 ReLU activation 0.25 FLOPS/B memory 5 Layer normalization < 10 FLOPS/B memory For #2 I get: Flops =  2 ops * 1024 inputs * 4096 weights each Memory =  2 bytes * (1024 input reads + 1024*4096 weight reads + 4096 write outputs) --> Flops/Memory ~= 1 Flops/B What about the remaining operations? Each of the output activation requires 1024x2 = 2048 MACs.. (ignoring additions) i.e. 1024 weights need to be multiplied by 1024 input activation to produce one output activation. And there are 4096 output activations i.e. 2048*4096 = 8388608 MACs and batch size is 512 i.e. 4294967296 MACs For memory accesses, we need to access 1024 input activations (read), 4096 output activations (write) for a batch size of 1, but we have a batch size of 512, each of these operands is assumed to be FP16. so, for input and output activations, we have input accesses = 1024 * 512 * 2 = 1048576 output accesses = 4096 * 512 * 2 = 4194304 weights = 1024 * 4096 * 2 = 8388608 totaling to 13631488 and ratio of 4294967296 MACs / 13631488  bytes = 315