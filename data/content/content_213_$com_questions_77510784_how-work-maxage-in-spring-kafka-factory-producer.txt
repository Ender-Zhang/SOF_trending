We are using transactional producers and sometimes we find ourselves in the situation where there is no traffic for more than 7 days, which leads to the loss of transactional id metadata. The next write after this time always causes an error: workaround for now is to restart all instances in Kubernetes. Reviewing the documentation, I see that in version 2.5.8 a new property 'maxAge' has been added, which must be less than 7 days to be able to refresh the metadata on the broker side. I was testing and there is no way to make it work as expected. I have set up a demo project to simulate the behavior, except for some error on my part, maxAge does not refresh metadata before the time of: Versions used: confluentinc/cp-kafka:latest - Kafka 3.4 Spring Boot 3.1.5 JDK 17 Kafka Clients 3.4.1 Spring Kafka 3.0.12 Spring Stream Binder Kafka 4.0.4 The transaction metada is forced to expire in 10 seconds. Set the maxAge to 7 seconds, with the expectation that it will refresh before 10 seconds to gain an additional 10 seconds: Test Controller to producer message on demand I expect to never encounter the InvalidPidMappingException, but I always see it. If I don't handle the ABORTABLE_ERROR (AfterRollbackProcessor) or DLQ, I also lose messages. What am I doing wrong? Github Demo Project: https://github.com/Fonexn/kafkaMaxAgeTesting Thanks SOLUTION Set maxAge to each binder in this way, example: maxAge does not refresh the metadata. When a transactional producer is retrieved from the cache, if the maxAge has been exceeded, the producer is closed and we examine the next one in the cache. This continues until a young producer is found or the cache is empty. If all the producers in the cache are too old, a new producer is created. See createTransactionalProducer() and expire() .