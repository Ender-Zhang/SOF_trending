I have written a code for a custom transformer trained on some dataset. My issue lies with the model not utilizing my gpu even though i have initialized everything in my code to my best knowledge. Can someone help me figure out why the cpu is being used for training ? I am using kaggle's P100 gpu as my accelerator. Simply the gpu is not being used, since its a relatively large model to be trained on a cpu i dont want to spend hours waiting for it to be trained on the cpu.