I have a 25GB pickle of a dictionary of numpy arrays.
The dictionary looks like the following: 668,956 key-value pairs. The keys are strings.  Example key: "109c3708-3b0c-4868-a647-b9feb306c886_1" The values are numpy arrays of shape 200x23 , type float64 When I load the data using pickle repeatedly in a loop, the time to load slows down (see code and result below).  What could be causing this? Code: Result: Notes: During the processing of the loop the RAM usage ramps down (I assume dereferencing the previous data in the file variable), before ramping up again.  Both unloading and loading parts seem to slow down over time.  It surprises me how slow the RAM decreases in the unloading part. The total RAM usage it ramps up to stays about constant (it doesn't seem like there's a memory leak). I've tried including del file and gc.collect() in the loop, but this doesn't speed anything up. If I change return pickle.load(handle) to return handle.read() , the unload time is consistently 0.45s and load time is consistently 4.85s. I'm using Python 3.9.13 on Windows with SSD storage ( Python 3.9.13 | packaged by conda-forge | (main, May 27 2022, 16:51:29) [MSC v.1929 64 bit (AMD64)] ). I have 64GB RAM and don't seem to be maxing this out. Why am I doing this?  During training of an ML model, I have 10 files that are each 25GB big.  I can't fit them all into memory simultaenously, so have to load and unload them each epoch. Any ideas?  I'd be willing to move away from using pickle too if there's an alternative that has similar read speed and doesn't suffer from the above problem (I'm not worried about compression). Edit:
I've run the above loading and unloading loop for different sized pickles. Results below showing the relative change in speed over time. For anything above 3 GB, the unload time starts to significantly ramp.