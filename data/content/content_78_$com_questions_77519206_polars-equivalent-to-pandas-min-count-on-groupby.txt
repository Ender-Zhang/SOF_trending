I'm trying to find the equivalent of a min_count param on polars groupby, such as in pandas.groupby(key).sum(min_count=N) . Let's suppose the dataframe How can I groupby through the fruit key with the constrain of the group having at least 4 values for the sum? So instead of I'd have only fruit b on the output, since it's the only one with at least 4 elements I don't think there's a built-in min_count for this, but you can just filter: I had not seen min_count before, so incase it's useful to readers: min_count appears to specifically deal with NaN values The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. Your example is using "group length ", but if you require "not nan count" or "not null count" it will require some extra logic. It does also raise an interesting question: "Does Polars have a .group_by().filter() ?" Currently, I think you need 2 operations, e.g. you could also filter before aggregation: Depending on the aggregation, and the filter condition, it's possible that filtering first could be more "efficient". But I'm not sure how Polars treats the 2 different approaches "internally". Another way to do this which is likely worse performance than the group_by.agg.filter method is to use a window function in the filter before the group_by. If the data is big AND the filter is very restrictive AND the operations in the agg are expensive this might be faster but otherwise it'll be slower since it's essentially doing the grouping twice. I did a test with varying sizes of df, filter size, and agg complexity to see the relative performance. big/small refers to the filter size where big means the filter results were roughly 5/8ths of the n and small means about 1/8th. grouped/windowed is the difference between the polars expression (ie grouped is ignoring_gravity's method and windowed is the above) sum/pow is either pl.col('value').sum() or pl.col('value').pow(3.2).sum() in the agg You can quickly see that the best performance is from the grouped sum because sum is cheap and it's only grouping once. It might be hard to see but the big/small grouped pow are also essentially a tie. This shouldn't be surprising since the computations happen before the filter. The worst performance is the big_windowed_pow because it's getting the worst of both worlds, it's having to do the grouping twice and it's having to do the expensive operation on a big portion of the remaining rows. Here's a chart of the fraction of the time small_windowed_pow takes over small_grouped_pow When you get over a million then it's about 10% faster but is very sensitive to the size of the filter.