I'm working with TensorFlow-GPU version 2.10.0, and I'm facing challenges in achieving deterministic training results. I have set all relevant random seeds, including random.seed(42), np.random.seed(42), and tf.random.set_seed(42). Despite these seed settings, I notice that the accuracy results vary increasingly with each epoch during training. Method1:
In my search for a solution, I came across the suggestion to use tf.config.experimental.enable_op_determinism(). However, when I attempt to implement this, I encounter error messages. Here's a summary of the seed-setting code snippet: And the attempt to enable op determinism: Error message: Method2: Setting TF_CUDNN_DETERMINISTIC to '1' in TensorFlow is used to enforce deterministic behavior in CuDNN (CUDA Deep Neural Network library) operations. When set to '1', it tries to make the computation reproducible by using deterministic algorithms in CuDNN. Let use_multiprocessing=False The initial weights are consistently the same each time, but discrepancies in training results still emerge after the first epoch. Experiment 1: Experiment 2: I train the model on the CPU, and I can achieve identical results each time (identical accuracy and loss), but the speed is slow. Since my application requires running the model training process extensively, I still hope to speed it up. I would like to know achieve consistent training results across epochs with TensorFlow-GPU 2.10.0? Are there alternative approaches or solutions to address this issue?