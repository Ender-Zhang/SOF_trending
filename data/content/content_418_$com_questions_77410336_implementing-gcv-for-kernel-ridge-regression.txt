I'm implementing Kernel Ridge Regression in the R language. Specifically, I want to determine the optimal value of lambda using the generalized cross-validation (GCV), but I'm facing some issues. I've written the following code: The code calculates the optimal lambda as 1e-13, but when I look at the attached plot, it doesn't seem to be the best choice. For instance, when I set lambda to 1e-2, it appears to fit the data better (please refer to the plot below). enter image description here What could be the cause of this phenomenon in my code?
I would appreciate it if you could provide some insight.