Disclaimer (1): This question is supportive to this SO . After a request from two users to elaborate on my case. Disclaimer (2) - added 29/11: I have seen two solutions so far (proposed in this SO and the supportive one), that utilize the explode() functionality. Based on some benchmarks I did on the whole (~3m rows data) the RAM literally explodes , thus I will test the function on a sample of the dataset and if it works I will accept the solutions of explode() method for those who might experiment on smaller tables. The input dataset (~3m rows) is the ratings.csv from the ml-latest dataset of 80_000 IMDb movies and respective ratings from 330_000 users (you may download the CSV file from here - 891mb). I load the dataset using polars like movie_ratings = pl.read_csv(os.path.join(application_path + data_directory, "ratings.csv")) , application_path and data_directory is a parent path on my local server. Having read the dataset my goal is to generate the cosine similarity of a user between all the other users. To do so, first I have to transform the ratings table (~3m rows) to a table with 1 row per user. Thus, I run the following query The code snippet above groups the table by userId and computes some important metadata about them. Specifically, user_movies, user_ratings per user common_movies = intersection of the movies seen by the user that are the same as seen by the input_id user (thus user 1). Movies seen by the user 1 are basically user_rated_movies = movie_ratings.filter(pl.col("userId") == input_id).select("movieId").to_numpy().ravel() common_movies_frequency = The length of the column common_movies per user. NOT a fixed length per user. common_movie_ratings = The result of the function I asked here target_user_common_movie_ratings = The ratings of the target user (user1) that match the indexes of the common movies with each user. similarity_score = The cosine similarity score. Screenshot of the table (don't give attention to column potential recommendations ) Finally, I filter the table users_metadata by all the users with less than or equal common_movies_frequency to the 62 ( len(user_rated_movies) ) movies seen by user1. Those are a total of 250_000 users. This table is the input dataframe for the UDF function I asked in this question. Using this dataframe (~250_000 users) I want to calculate the cosine similarity of each user with user 1. To do so, I want to compare their rating similarity. So on the movies commonly rated by each user, compute the cosine similarity among two arrays of ratings. Below are the three UDF functions I use to support my functionality. Benchmarks Total execution time for 1 user is ~4 minutes. If I have to compute this over an iteration per user (1 dataframe per user) that will be approximately4 minutess * 330_000 users. 3-5Gb of RAM while computing the polars df for 1 user. The main question is how can I transform those 3 UDF functions into native polars commands. logs from a custom logger I made 2023-11-29 13:40:24 - INFO - Computed potential similar user metadata for 254188 users in: 0:02:15.586497 2023-11-29 13:40:51 - INFO - Computed similarity scores for 194943 users in: 0:00:27.472388 We can conclude that the main bottleneck of the code is when creating the user_metadata table. CSV pl.read_csv loads everything into memory. pl.scan_csv() returns a LazyFrame instead. Parquet faster to read/write pl.scan_csv("imdb.csv").sink_parquet("imdb.parquet") imdb.csv = 891mb / imdb.parquet = 202mb Example: In the hopes of making things simpler for replicating results, I've filtered the dataset pl.col("userId").is_between(1, 3) and removed the timestamp column: We will assume input_id == 1 One possible approach for gathering all the needed information: Lazy API There may be a better strategy to parallelize the work, but a baseline attempt could simply loop through each userID DuckDB I was curious, so decided to check duckdb for a comparison. RAM Usage Running both examples against the full dataset (runtime is basically the same) I get: So it seems there is potential room for improvement on the Polars side. Here is an approach I came up with, using a random df 1/100th the size of the given dataset: (I think userId & movieId should be unique in the actual dataset?) Then: This will put a 2999x2 dataframe per userId into the final dictionary. These can all be concatenated with an additional with_column(pl.lit(key_of_dict).alias('user2') or some such if one big final df is wanted. Note how: two columns of literals of the movieIds and user_ratings of a given user are added to the intermediate dataframe mr , very simple syntax here list_intersection is avoided simply by doing an explode to generate all combinations of movie/ratings between two users, and a filter on all rows that have matching movieId s I precompute the squares of the ratings because list.eval is considered slow When there's just one matching movie, we know the result will be 1, so short-circuit and save some calculations there All the non-matching users are added in at the end with a join and fill_null Generating all 3000 of these 2999x2 dfs took my machine 62 seconds - not really sure how feasible this will be at full size (and I'm not sure a reasonable solution exists for getting every userId combo, especially at that size), but at least it is all native polars! You can use numba to compile the similarity function into a ufunc which can be placed directly in an expression without using a call to map_* . Additionally, you can self join the df to itself to do all of the list intersection stuff. You can use pyarrow's parquet writer in order to write chunks of results to an open file without materializing it all at once. There are a bunch of commented out lines that I think are actually unnecessary given the parquet writer, which is why they're commented out. Essentially they just save bits to files to try to lessen the memory requirement. I played around with keeping the main df as a scan_parquet lazyframe but it slows it down a good bit since it constantly has to read the file and, on my computer, that individual file isn't too big for memory. It's the big join that chews up a lot of memory. I verified that, at least for userid==1 that the similarity score matches the result from compute_cosine . I let it run for about 11 minutes and in that time it did 584 users. It would go even faster if it weren't writing the results to disk after each user. There's probably an opportunity to set it up such that it only writes to disk after it's accumulated a few users worth of results but defining how many users that should be is system/memory dependent.