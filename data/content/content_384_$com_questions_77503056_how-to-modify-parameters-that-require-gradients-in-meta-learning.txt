I have a neural network that is trained to output learning rates: I have some inputs and a function I'm trying to learn: I'm trying to update one trainable parameter to solve this function: So the setting is that the meta model should learn to output the optimal learning rate to update the trainable parameter w1 based on the current loss. The issue is that I'm getting RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1]] is at version 2; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True). I also tried replacing the update step with w1.data = w1.data - meta_model_output * w1.grad # step , which resolves the issue, but then the meta model is not updating (i.e., the loss stays the same) Update 1: Tried @VonC idea of computing the updated value of w1 (using a clone of w1: w1_updated_value) and setting it as the data of w1: While this removes the error, it results in the same issue of the meta model is not updating (i.e., the loss stays the same). Update 2: After lots of readings on buffers and updating leaf tensors I got a solution that updates a whole network: Though, 1) I'm not sure why this works as the update step seems similar to what I tried. 2) Since I don't understand why this works I can't figure out how to use this on my initial setup where I have a simple parameter w1 that is not a whole network Update 3: Tried @VonC suggestion on wrapping my tensor of weights as a buffer: But I'm still getting the inplace operation runtime error.