I'm trying to use TensorFlow.js to extract feature embeddings from images. Elsewhere I'm using PyTorch and ResNet152 to extract feature embeddings to good effect. The following is a sample of how I'm extracting those feature embeddings. Essentially, I'm using the pre-trained model and dropping the last layer to get my feature embeddings. The result of the above script is: So now, I want to do something similar with TensorFlow.js. The first thing that I need is an instance of the ResNet152 model that I can use with TensorFlow.js. So I created the following Python script to export ResNet152 to the Keras format... And then I exported the Keras (.h5) model to the TensorFlow.js format using the "tensorflowjs_converter" utility... Once I have the model in the appropriate format (I think), I switch over to Javascript. The result of the above script is: Looking at the first three values of the outputs between the two versions of the script, I expected there to be some variation but not THIS MUCH. Where do I go from here? Is this much variation expected? Am I just doing this wrong? Any help would be greatly appreciated. The associated Keras issue 18810 "Differences in feature embeddings in Keras and torch models" was just closed with: The first thing to check is whether you have the right preprocessing. The best practice is to always use keras.applications.xxxx.preprocess_input , in this case keras.applications.resnet.preprocess_input . I would not expect intermediate features or predictions to necessarily be close, because many of the models were retrained from scratch. You issue might have partly came from differences in how the images are preprocessed before being fed into the models. By standardizing the preprocessing step using the recommended method for Keras models, you could make sure the input is in the optimal format for that specific model, reducing one potential source of discrepancy. Regarding your concern about the significant variation in the feature embeddings between the PyTorch and Keras models, different implementations of the same model architecture (like ResNet152 in this case) might not produce closely similar intermediate features or predictions. Many models in different frameworks are retrained from scratch, leading to differences in learned features despite having the same architecture. That would help to understand that such variations in outputs are common and expected due to differences in the training process, even when the underlying architecture is the same.