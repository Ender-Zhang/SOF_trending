I'm trying to aggregate some rows in my dataframe with a list[str] column. For each index I need the intersection of all the lists in the group. Not sure if I'm just overthinking it but I can't provide a solution right now. Any help please? I've tried some stuff without success Another one I'm not sure if this is as simple as it may seem. You could get rid of the lists and use "regular" Polars functionality. One way to check if a value is contained in each row of the idx group is to count the number of unique (distinct) row numbers per idx, values group. You can filter those, and build the list of results with .group_by The issue with the fold/reduce approach is that the expressions parameter needs a list/iterator of actual expressions or at least an expression that knows it refers to multiple columns like pl.all() . It's not going to work as an aggregation. You can work around that by segmenting your df by the length of each idx and then using a comprehension to deliver the expressions. This works a lot like a pivot where we make a wide df and then refer to each column as an expression but instead of an outright pivot it creates a list of lists then just uses get on each of those elements as if they were columns. Alternate syntax As a response to @jqurious's comment. You can use group_by instead of partition_by but then you have to use map_groups with a custom function. That might look like this: Note that the first line requires a collect of the n because we need the list comprehension in reduce s expr parameter to know n_count in python terms. That was one of the nice benefits of using partition_by is that we got that through using a as_dict . I presume the benefit of group_by is that you can then start with a Lazyframe which then requires that the schema of map_groups be given to it like this: Second Alternate syntax If we convert the values list to a struct then we don't need to know how many there are, it's good enough that we know there's a unified number of them. In that way you could do it like this... I'm not sure how this would parallelize so if someone wants to do a benchmark between the two with bigger starting data, it would certainly be interesting. This should work