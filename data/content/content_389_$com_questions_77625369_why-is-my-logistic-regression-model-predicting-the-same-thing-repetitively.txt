https://www.kaggle.com/datasets/akshaydattatraykhare/diabetes-dataset I used this kaggle dataset for my diabetes dataset and am trying to create a LogisticRegression model to predict outcomes. I created the following class: It is repetitively outputting: It has been outputting the "Diabetes" outcome for all the predictions it has made. I'm new to machine learning but I know that I haven't tuned my hyperparameters. Does it have to do with the length of the dataset, is it too short? Or maybe something to do with my k fold cross validation? In this case, I don't think the issue is with the dataset, as it has a 10.0 usability rating on Kaggle. The only thing that stands out to me in your code is the 2000 iterations that you train the model for, it could be a bit too much and cause the model to overfit. If you don't want to mess with hyperparameter tuning yet, try lowering the number of iterations yourself by different intervals, and see how the model behavior changes with different values of max_iter . If you want to find the optimal number of iterations, try using GridSearchCV from sklearn ( docs ) on the max_iter hyperparameter in order to find the best number of iterations to train the model for. I think it helps to start with a simple baseline. In your case there's additional complexity of wrapping the model in a class. Here's a simpler approach that is useful for checking things are working, and running some simple tests: We start off without using CV, just to ensure the model is fitting and we're getting reasonable results. The 0 predictions look good, but the 1 predictions are quite spread out. A histogram of the data identifies potential issues: Some features have a value of 0. Whilst this makes sense for some features like Pregnancies, it's not a valid value for Age, BMI, and other measurements. There are ways to deal with this, including replacing those invalid zeros with the mean. Running .hist() on y_train shows that the dataset is imbalanced - using class_weight="balanced" will help LogisticRegression manage the imbalance.