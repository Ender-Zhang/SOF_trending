My input dataframe (or 2D numpy array) shape is 11kx12k For each element, I check how many values are higher in the given row and column. For example: row-wise: column-wise: total higher values for that element: this code works good but for this shape of matrix It takes ~1 hour. I wonder is there any way to do this more efficient way? I also tried numpy but for this I have splited my 2D to 12kx100 or 12kx200 shapes and merged all arrays again, in the end runtime was close to eachother so couldn't get any progress. I came up with the following: What you want is similar to a ranking of elements, which basically tells you how many items are smaller than a given element (maybe with subtracting 1 if your ranking is 1, 2, 3, ... ). rankdata does this ranking, and can do it row-wise or column-wise.  We can convert the "smaller than" to "bigger than" by inverting the ranks, relative to the number of items being counted (in each row/column).  I.e., subtract the ranks from the number of rows/columns. For rows/columns without ties (e.g., the third row in your example), this works as expected.  There is a slight complication with ties and how they are ranked.  Through trial and error, I found that using method='max' gave the desired behavior.  To be frank, I am still wrapping my head on why/how this works, so take the answer with a grain of salt. Still, it gives the desired output on the input data: And it can handle your size of data in far less than an hour: Takes about 10 seconds. My understanding is the ranking should be similar to a sorting operation.  My answer is relying on a scipy/numpy implementation of the rank/sort, rather than a user-written loop/"apply" (which tend to be much slower). Another possible solution, where a is the initial array, which uses row-wise numpy broadcasting : The previous solution takes a long time to run. Thus, to speed up the calculations, we can use numba and numba prange to parallelize the for loop: This code took about 1 minute and 30 seconds to run. Output: