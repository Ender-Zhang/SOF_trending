I have two lists, both contain integers represented as strings. large_list is large, up to low 10s of thousands of elements. small_list is smaller, up to hundreds of elements. I am filtering the larger one by the smaller one. I'm curious to know which is more efficient: (a) (b) for option (a), the iteration count is limited by the length of small_list , but for each one, a search has to be executed on the full large_list .
for option (b), the iteration goes through the entire large_list but each search is executed on small_list . I understand that sets are hashed, so the best option here ought to be to take large_set = set(large_list) , iterate through small_list , and do each search on large_set . But if we take sets out of the equation, is option (a) more efficient, or is option (b)? You can speculate, you can calculate On my machine: (a) would be much more efficient because its only ever checking until a match is found. Output: (b) is much slower because it depends on the length of both lists because it will check every single element against each other in both lists. Output: Hope this helps! Sorry to be this guy, but current answers are flawed: I think @CDubyuh got explanation a bit wrong. In both cases checking "is in list" is stopped when match is found. @mugiseyebrows is not making fair test because you need to shuffle the list first, and make several repetitions using different lists. Probability of finding element of one list in another It is (generally, not in every case) more likely to find element from small list in a large list and stop checking "in the middle" than it is to find element from large list in a small list. Lets take example from mugiseyebrows, where 50% of small list values are in the large list. Will also fix it a bit by adding random shuffling and test repetitions. If we make small list subset of large, the differences are bigger, as expected: number of calls to in operateor Also, there are more calls to in operator in the second case which will have its effect on performance too. To test how it affects the performance, let's create test when small and large lists have no common values. As you can see difference between both cases is much smaller. Still small_in_large is ~15% faster Summary Small_in_large is always faster. The bigger is joint subset of values between the lists, the more performant small_in_large will be. Internal C code (e.g. the in comparison operator) is always going to be faster than iterating using python code.  In this case the outer for-loop (python code) is your bottleneck so, even without measuring it, using the smaller list in the for-loop is going to be more efficient. There are various ways to get better performance using more of the built-in functions (C code) in your main loop: Keep in mind that, if your lists contain duplicates, these and even your original variants may not give the same result If you abstract out the difference between C-code and Python code (as would be the case in the 1st filter example above) and exclude specific internal optimizations, in theory there would not by any difference.  Assuming a regular distribution, finding an item in the large list would take L/2 iterations on average (L being its length) and finding an item in the small one would take S/2 iterations.   Multiplying by the outer loop's iteration count gives either S*(L/2) or L*(S/2).  Because multiplication is commutative, both give the same number of iterations (S*L/2).
Actual data and internal code are rarely that regular/symmetrical so, depending on what you are working with, you will need to make choices that fit the context and/or apply to the majority of use cases.