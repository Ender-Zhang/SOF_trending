I'm trying to build an NLP model that uses XGBoost. In my following code. What I don't understand is, I previously trained my dataset with 10000+ data and got good results. I then decide to save the model so that I can make predictions using user data. My model detects whether a text document is fraudulent or real. I have a dataset thats labelled for fraudulent data. I understand that when transforming data, vectorizer and pca should both be fitted to our whole dataset so that it will result in the same shape. What I dont understand is, how do I make it such that, I can transform user input, and have it the same shape as my model that I pretrained? Whats the proper procedure for this? Would love answers that also consider performance/time needed to process the data. This is done automatically as part of the CountVectorizer - tokens which appear in a new dataset but not in the data it is fit upon are simply ignored and the shape of the output will remain the same. For example: and cat cats hat in like milk rats the 0 1 0 1 1 0 0 0 2 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 0 1 0 Now if we pass new data to the fitted CountVectorizer that includes tokens it hasn't seen before (birds, dogs) they are ignored, and the dimensionality of the document-term matrix remains the same: and cat cats hat in like milk rats the 1 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 Since unseen tokens are ignored, this stresses the importance of retraining and/or consistency in the distribution of tokens in your training data and any data the model is used upon. Additionally, I would also avoid using floating point value for n_components for PCA but instead pick a set number of components (pass an integer value as opposed to a float) so that the output dimensionality of preprocessing is consistent.